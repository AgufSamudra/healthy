{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# alodokter cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\gufra\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "Processing hidup-sehat: 100%|██████████| 3012/3012 [00:19<00:00, 151.97file/s]\n",
      "Processing keluarga: 100%|██████████| 2386/2386 [00:15<00:00, 150.73file/s]\n",
      "Processing kesehatan: 100%|██████████| 2215/2215 [00:14<00:00, 149.74file/s]\n",
      "Processing obat-a-z: 100%|██████████| 1348/1348 [00:09<00:00, 137.22file/s]\n",
      "Processing penyakit-a-z: 100%|██████████| 1233/1233 [00:08<00:00, 137.27file/s]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import string\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Download NLTK resources jika belum dilakukan sebelumnya\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Daftar folder untuk dilakukan looping\n",
    "folders = ['hidup-sehat', 'keluarga', 'kesehatan', 'obat-a-z', 'penyakit-a-z']\n",
    "\n",
    "# Path dasar untuk direktori sumber dan tujuan\n",
    "src_base_path = 'dataset/data_ex_3/alodokter/'\n",
    "dest_base_path = 'dataset/data_ex_3_cleaning/alodokter/'\n",
    "\n",
    "# Fungsi untuk membersihkan teks dari tanda baca menggunakan NLTK\n",
    "def clean_text(text):\n",
    "    text = text.lower().strip()  # Mengubah menjadi huruf kecil dan menghapus spasi di awal dan akhir\n",
    "    translator = str.maketrans('', '', string.punctuation)  # Membuat translator untuk menghapus tanda baca\n",
    "    text = text.translate(translator)  # Menghapus tanda baca\n",
    "    return text\n",
    "\n",
    "for folder in folders:\n",
    "    src_folder_path = os.path.join(src_base_path, folder)\n",
    "    \n",
    "    # Buat folder tujuan jika belum ada\n",
    "    dest_folder_path = os.path.join(dest_base_path, folder)\n",
    "    if not os.path.exists(dest_folder_path):\n",
    "        os.makedirs(dest_folder_path)\n",
    "\n",
    "    # Lakukan looping pada semua file .txt dalam folder sumber\n",
    "    file_list = os.listdir(src_folder_path)\n",
    "    progress_bar = tqdm(file_list, desc=f'Processing {folder}', unit='file')\n",
    "\n",
    "    for filename in progress_bar:\n",
    "        if filename.endswith('.txt'):\n",
    "            src_file_path = os.path.join(src_folder_path, filename)\n",
    "            dest_file_path = os.path.join(dest_folder_path, filename)\n",
    "\n",
    "            # Baca konten dari file sumber\n",
    "            with open(src_file_path, 'r', encoding='utf-8') as src_file:\n",
    "                content = src_file.read()\n",
    "\n",
    "                # Membersihkan teks dari tanda baca, membuat huruf kecil, dan menghapus spasi di awal dan akhir\n",
    "                cleaned_content = clean_text(content)\n",
    "\n",
    "                # Menulis konten yang sudah dibersihkan ke file tujuan\n",
    "                with open(dest_file_path, 'w', encoding='utf-8') as dest_file:\n",
    "                    dest_file.write(cleaned_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Halodoc Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\gufra\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "Processing halodoc: 100%|██████████| 1419/1419 [00:10<00:00, 140.58file/s]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import string\n",
    "import nltk\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Download NLTK resources jika belum dilakukan sebelumnya\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Fungsi untuk membersihkan teks dari tanda baca menggunakan NLTK\n",
    "def clean_text(text):\n",
    "    text = text.lower().strip()  # Mengubah menjadi huruf kecil dan menghapus spasi di awal dan akhir\n",
    "    translator = str.maketrans('', '', string.punctuation)  # Membuat translator untuk menghapus tanda baca\n",
    "    text = text.translate(translator)  # Menghapus tanda baca\n",
    "    return text\n",
    "\n",
    "# Folder untuk dilakukan looping\n",
    "folder = 'halodoc'\n",
    "\n",
    "# Path dasar untuk direktori sumber dan tujuan\n",
    "src_base_path = 'dataset/data_ex_3/'\n",
    "dest_base_path = 'dataset/data_ex_3_cleaning/'\n",
    "\n",
    "src_folder_path = os.path.join(src_base_path, folder)\n",
    "\n",
    "# Buat folder tujuan jika belum ada\n",
    "dest_folder_path = os.path.join(dest_base_path, folder)\n",
    "if not os.path.exists(dest_folder_path):\n",
    "    os.makedirs(dest_folder_path)\n",
    "\n",
    "# Lakukan looping pada semua file .txt dalam folder sumber\n",
    "file_list = os.listdir(src_folder_path)\n",
    "progress_bar = tqdm(file_list, desc=f'Processing {folder}', unit='file')\n",
    "\n",
    "for filename in progress_bar:\n",
    "    if filename.endswith('.txt'):\n",
    "        src_file_path = os.path.join(src_folder_path, filename)\n",
    "        dest_file_path = os.path.join(dest_folder_path, filename)\n",
    "\n",
    "        # Baca konten dari file sumber\n",
    "        with open(src_file_path, 'r', encoding='utf-8') as src_file:\n",
    "            content = src_file.read()\n",
    "\n",
    "            # Membersihkan teks dari tanda baca, membuat huruf kecil, dan menghapus spasi di awal dan akhir\n",
    "            cleaned_content = clean_text(content)\n",
    "\n",
    "            # Menulis konten yang sudah dibersihkan ke file tujuan\n",
    "            with open(dest_file_path, 'w', encoding='utf-8') as dest_file:\n",
    "                dest_file.write(cleaned_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# News Health Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\gufra\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import string\n",
    "\n",
    "# Download NLTK resources jika belum dilakukan sebelumnya\n",
    "nltk.download('punkt')\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Fungsi untuk membersihkan teks dari tanda baca menggunakan NLTK\n",
    "def clean_text(text):\n",
    "    text = text.lower().strip()  # Mengubah menjadi huruf kecil dan menghapus spasi di awal dan akhir\n",
    "    translator = str.maketrans('', '', string.punctuation)  # Membuat translator untuk menghapus tanda baca\n",
    "    text = text.translate(translator)  # Menghapus tanda baca\n",
    "    return text\n",
    "\n",
    "# Lokasi file CSV\n",
    "csv_file = 'dataset/data_ex_3/news_health/cnn_indonesia_health.csv'\n",
    "\n",
    "# Baca file CSV\n",
    "df = pd.read_csv(csv_file)\n",
    "\n",
    "# Path dasar untuk direktori tujuan\n",
    "dest_base_path = 'dataset/data_ex_3_cleaning/news_health/'\n",
    "\n",
    "# Buat folder tujuan jika belum ada\n",
    "if not os.path.exists(dest_base_path):\n",
    "    os.makedirs(dest_base_path)\n",
    "\n",
    "# Lakukan looping pada setiap baris dalam DataFrame\n",
    "for index, row in df.iterrows():\n",
    "    # Ambil judul dan konten\n",
    "    title = row['judul']\n",
    "    content = row['content']\n",
    "\n",
    "    # Check if title is a string\n",
    "    if isinstance(title, str):\n",
    "        # Bersihkan judul menggunakan NLTK\n",
    "        title = clean_text(title)\n",
    "\n",
    "        # Buat nama file dari judul\n",
    "        filename = f\"{title}.txt\"\n",
    "        file_path = os.path.join(dest_base_path, filename)\n",
    "\n",
    "        # Check if content is a string before writing to file\n",
    "        if isinstance(content, str):\n",
    "            # Bersihkan konten menggunakan NLTK\n",
    "            content = clean_text(content)\n",
    "\n",
    "            # Tulis konten ke file\n",
    "            with open(file_path, 'w', encoding='utf-8') as file:\n",
    "                file.write(content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# QNA Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data telah dibersihkan dan disimpan di folder cleaning.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "\n",
    "# Baca file CSV\n",
    "df = pd.read_csv('dataset/data_ex_3/qna_alodokter/chat_health.csv')\n",
    "\n",
    "# Bersihkan data HTML dan ubah ke huruf kecil\n",
    "df['jawab'] = df['jawab'].apply(lambda x: BeautifulSoup(x, 'html.parser').get_text().lower().strip())\n",
    "\n",
    "# Pastikan folder 'cleaning' ada\n",
    "if not os.path.exists('dataset/data_ex_3_cleaning/qna_alodokter/'):\n",
    "    os.makedirs('dataset/data_ex_3_cleaning/qna_alodokter/')\n",
    "\n",
    "# Simpan ke file TXT dengan encoding 'utf-8'\n",
    "with open('dataset/data_ex_3_cleaning/qna_alodokter/data_bersih.txt', 'w', encoding='utf-8') as file:\n",
    "    for index, row in df.iterrows():\n",
    "        file.write(row['jawab'] + '\\n')\n",
    "        file.write('==================' + '\\n')\n",
    "\n",
    "print('Data telah dibersihkan dan disimpan di folder cleaning.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merge Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Semua file telah digabungkan ke dalam dataset/data_ex_3_cleaning/alodokter_article.txt\n"
     ]
    }
   ],
   "source": [
    "# ALODOKTER ARTICLE\n",
    "\n",
    "import os\n",
    "\n",
    "# Tentukan path ke direktori utama di mana subfolder 'alodokter' berada\n",
    "root_dir = 'dataset/data_ex_3_cleaning/'\n",
    "\n",
    "# Path ke subfolder 'alodokter'\n",
    "alodokter_dir = os.path.join(root_dir, 'alodokter')\n",
    "\n",
    "# Mencari semua subfolder di dalam direktori 'alodokter'\n",
    "subfolders = [os.path.join(alodokter_dir, o) for o in os.listdir(alodokter_dir) \n",
    "              if os.path.isdir(os.path.join(alodokter_dir, o))]\n",
    "\n",
    "# File tempat semua konten akan digabungkan\n",
    "output_file = os.path.join(root_dir, 'alodokter_article.txt')\n",
    "\n",
    "# Membuka file output untuk menulis\n",
    "with open(output_file, 'w', encoding='utf-8') as outfile:\n",
    "    # Iterasi melalui setiap subfolder\n",
    "    for folder in subfolders:\n",
    "        # Iterasi melalui setiap file dalam subfolder\n",
    "        for filename in os.listdir(folder):\n",
    "            if filename.endswith('.txt'):\n",
    "                # Path ke file txt\n",
    "                file_path = os.path.join(folder, filename)\n",
    "                \n",
    "                # Membuka file txt dengan encoding 'utf-8' dan menulis isinya ke file output\n",
    "                with open(file_path, 'r', encoding='utf-8') as readfile:\n",
    "                    outfile.write(f\"file {filename}\\n\\n\")\n",
    "                    outfile.write(readfile.read())\n",
    "                    outfile.write(\"\\n\\n=========================\\n\\n\")\n",
    "\n",
    "print(f\"Semua file telah digabungkan ke dalam {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Semua file telah digabungkan ke dalam dataset/data_ex_3_cleaning/halodoc\\halodoc_article.txt\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Tentukan path ke direktori 'halodoc'\n",
    "halodoc_dir = 'dataset/data_ex_3_cleaning/halodoc'\n",
    "\n",
    "# File tempat semua konten akan digabungkan\n",
    "output_file = os.path.join(halodoc_dir, 'halodoc_article.txt')\n",
    "\n",
    "# Membuka file output untuk menulis\n",
    "with open(output_file, 'w', encoding='utf-8') as outfile:\n",
    "    # Iterasi melalui setiap file dalam direktori 'halodoc'\n",
    "    for filename in os.listdir(halodoc_dir):\n",
    "        if filename.endswith('.txt'):\n",
    "            # Path ke file txt\n",
    "            file_path = os.path.join(halodoc_dir, filename)\n",
    "            \n",
    "            # Membuka file txt dengan encoding 'utf-8' dan menulis isinya ke file output\n",
    "            with open(file_path, 'r', encoding='utf-8') as readfile:\n",
    "                outfile.write(f\"file {filename}\\n\\n\")\n",
    "                outfile.write(readfile.read())\n",
    "                outfile.write(\"\\n\\n=========================\\n\\n\")\n",
    "\n",
    "print(f\"Semua file telah digabungkan ke dalam {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Semua file telah digabungkan ke dalam dataset/data_ex_3_cleaning/news_cnn.txt\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Tentukan path ke direktori 'news_cnn'\n",
    "news_cnn_dir = 'dataset/data_ex_3_cleaning/news_health'\n",
    "\n",
    "# File tempat semua konten akan digabungkan\n",
    "output_file = os.path.join(\"dataset/data_ex_3_cleaning/\", 'news_cnn.txt')\n",
    "\n",
    "# Membuka file output untuk menulis\n",
    "with open(output_file, 'w', encoding='utf-8') as outfile:\n",
    "    # Iterasi melalui setiap file dalam direktori 'news_cnn'\n",
    "    for filename in os.listdir(news_cnn_dir):\n",
    "        if filename.endswith('.txt'):\n",
    "            # Path ke file txt\n",
    "            file_path = os.path.join(news_cnn_dir, filename)\n",
    "            \n",
    "            # Membuka file txt dengan encoding 'utf-8' dan menulis isinya ke file output\n",
    "            with open(file_path, 'r', encoding='utf-8') as readfile:\n",
    "                outfile.write(f\"file {filename}\\n\\n\")\n",
    "                outfile.write(readfile.read())\n",
    "                outfile.write(\"\\n\\n=========================\\n\\n\")\n",
    "\n",
    "print(f\"Semua file telah digabungkan ke dalam {output_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from itertools import chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = TextLoader(\"dataset/data_ex_3_cleaning/alodokter/alodokter_article.txt\", encoding=\"utf-8\")\n",
    "docs = loader.load()\n",
    "\n",
    "loader2 = TextLoader(\"dataset/data_ex_3_cleaning/halodoc/halodoc_article.txt\", encoding=\"utf-8\")\n",
    "docs2 = loader2.load()\n",
    "\n",
    "loader3 = TextLoader(\"dataset/data_ex_3_cleaning/news_health/news_cnn.txt\", encoding=\"utf-8\")\n",
    "docs3 = loader3.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "117898"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from itertools import chain\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=250,\n",
    "    separators=[\"\"])\n",
    "\n",
    "texts = text_splitter.split_documents(docs)\n",
    "texts2 = text_splitter.split_documents(docs2)\n",
    "texts3 = text_splitter.split_documents(docs3)\n",
    "\n",
    "list_data = list(chain(texts, texts2, texts3))\n",
    "\n",
    "len(list_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\codingproject\\healthybot\\env\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "\n",
    "embeddings = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\", google_api_key=\"AIzaSyDpEh8S4jo__bjNtJy2hN9cX838FZyF4Ww\")\n",
    "\n",
    "persist_directory = \"db_V2_noQA\"\n",
    "vectordb = Chroma.from_documents(documents=list_data,\n",
    "                                 embedding=embeddings,\n",
    "                                 persist_directory= persist_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "healthy_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
